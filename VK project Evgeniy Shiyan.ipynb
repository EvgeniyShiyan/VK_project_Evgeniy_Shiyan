{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, logging\n",
    "\n",
    "logging.set_verbosity_error()  # Ignore warning on model loading.\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# I will compare 3 models in my project: BERT, BERT + Logreg, BERT + RandomForest and then use the best one to score test data\n",
    "\n",
    "# Bert is an excellent model for such a task. I'm sure that the naive Bayesian classifier, usage of TF-IDF, Bag-of-Words\n",
    "# with classic models will be much worse so I decided to compare BERT in 3 variants (alone, with lr, with rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data to work with them later\n",
    "\n",
    "train_spam = pd.read_csv(r'train_spam.csv')\n",
    "test_spam = pd.read_csv(r'test_spam.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "train_spam['text_type'] = label_encoder.fit_transform(train_spam['text_type']) # 1 will be spam and 0 will be ham (not spam)\n",
    "train_texts = train_spam['text'].tolist()\n",
    "train_labels = train_spam['text_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2518ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analytics:\n",
    "\n",
    "# Derive information from dataframes\n",
    "print(train_spam.info())\n",
    "print(test_spam.info())\n",
    "\n",
    "# Look at the first 5 rows\n",
    "print(train_spam.head())\n",
    "print(test_spam.head())\n",
    "\n",
    "# Find out the number of messages with and without spam in the train\n",
    "print(train_spam['text_type'].value_counts())\n",
    "\n",
    "# Consider the average length of the text for spam and non-spam messages\n",
    "spam_text_len = train_spam[train_spam['text_type'] == 'spam']['text'].str.len().mean()\n",
    "not_spam_text_len = train_spam[train_spam['text_type'] != 'spam']['text'].str.len().mean()\n",
    "print(f\"the average length for spam: {spam_text_len:.2f}\")\n",
    "print(f\"the average length for non-spam: {not_spam_text_len:.2f}\")\n",
    "\n",
    "# Getting rows with missing values in any column (if any)\n",
    "print(train_spam[train_spam.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09943f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT\n",
    "\n",
    "tokenized_texts = bert_tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=500)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "batch_size = 32\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(train_texts), batch_size):\n",
    "        texts_batch = tokenized_texts[\"input_ids\"][i : i + batch_size].to(device)\n",
    "        masks_batch = tokenized_texts[\"attention_mask\"][i : i + batch_size].to(device)\n",
    "        output = bert_model(texts_batch, masks_batch)\n",
    "        batch_features = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        features.append(batch_features)\n",
    "\n",
    "features = np.concatenate(features, axis=0) # output which will be used further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting of models\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split # split our data\n",
    "train_labels = train_spam['text_type'].values\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Just BERT:\n",
    "# there is no need to add anything\n",
    "\n",
    "# BERT + LogisticRegression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.simplefilter('ignore')  # Ignore warning on model fitting.\n",
    "lr_clf = LogisticRegression().fit(train_features, train_labels)\n",
    "\n",
    "# BERT + RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.simplefilter('ignore')\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42).fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda296b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of models\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# just BERT:\n",
    "features_tensor = torch.from_numpy(features)\n",
    "probs = features_tensor.softmax(dim=-1)\n",
    "predicted_values = torch.zeros(len(test_features), dtype=torch.int64)\n",
    "for i in range(len(test_features)):\n",
    "    positive_prob = probs[i][1].item()  # Probability of positive class\n",
    "    if positive_prob > 0.5:\n",
    "        predicted_values[i] = 1 \n",
    "    else:\n",
    "        predicted_values[i] = 0\n",
    "auc = roc_auc_score(test_labels, predicted_values)\n",
    "print(\"Just BERT: \", auc)\n",
    "\n",
    "# BERT + LogisticRegression\n",
    "proba_lr = lr_clf.predict_proba(test_features)[:, 1]\n",
    "auc_lr = roc_auc_score(test_labels, proba_lr)\n",
    "print(\"BERT + LogisticRegression: \", auc_lr)\n",
    "\n",
    "# BERT + RandomForestClassifier\n",
    "label_pred_rf = rf_clf.predict(test_features)\n",
    "auc_rf = roc_auc_score(test_labels, label_pred_rf)\n",
    "print(\"BERT + RandomForestClassifier: \", auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT + LogisticRegression showed the highest quality on test data: roc_auc_score = 0.9869210901625936\n",
    "\n",
    "# Let's use it to score test_spam\n",
    "\n",
    "\n",
    "# To be honest, I started doing this task too late, so I didn't manage to fix the problem with a variant \"Just BERT\", \n",
    "# but 0.5 is definitely not normal auc for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring of test data\n",
    "\n",
    "test_texts = test_spam['text'].tolist() # the next part of code we have seen before when created output of BERT on train\n",
    "tokenized_test_texts = bert_tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=500)\n",
    "batch_size = 32\n",
    "features_test = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_texts), batch_size):\n",
    "        texts_test_batch = tokenized_test_texts[\"input_ids\"][i : i + batch_size].to(device)\n",
    "        masks_test_batch = tokenized_test_texts[\"attention_mask\"][i : i + batch_size].to(device)\n",
    "        output_test = bert_model(texts_test_batch, masks_test_batch)\n",
    "        batch_features_test = output_test.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        features_test.append(batch_features_test)\n",
    "\n",
    "features_test = np.concatenate(features_test, axis=0)\n",
    "proba_lr_test = lr_clf.predict_proba(features_test)[:, 1] # predict probas\n",
    "threshold = 0.5\n",
    "labels = np.where(proba_lr_test >= threshold, 1, 0)\n",
    "labels = ['spam' if label == 1 else 'not spam' for label in labels]\n",
    "\n",
    "# write the answer to a file\n",
    "answer_df = pd.DataFrame(test_spam['text'])\n",
    "answer_df['label'] = labels\n",
    "answer_df.to_csv('scoring_of_test_spam.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
